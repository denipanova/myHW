{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 0 HW2\n",
    "\n",
    "1. This assignment is an individual effort.\n",
    "2. Submission to be uploaded into your group repositories in the folder python_intro\n",
    "3. Deadline is 20th of April 5:00 PM.\n",
    "4. Please follow google's [python styleguide](https://google.github.io/styleguide/pyguide.html) for your code. Pay attention to the guidelines for naming convention, comments and main.\n",
    "5. Code will be checked for plagiarism. Compelling signs of a duplicated effort will lead to a rejection of submission and will attract a 100\\% grade penalty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "Please load the file provided to you by email. Use _json_ module to read it as a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995 Clinton\n",
      "1995\n",
      "Muh.aha\n",
      "140343895144928\n",
      "140343796943256\n",
      "[u'1995', u'Clinton', 'Muh.aha', ['even', 'a list']]\n",
      "[u'1995', u'Clinton', 'insert?', 'Muh.aha', ['even', 'a list']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "unicode"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 use open a connection to the file\n",
    "# 2 read contents of the file\n",
    "# 3 use the json module to read the string as a list\n",
    "#(replace None with relevant code)\n",
    "\n",
    "#import json module\n",
    "import json \n",
    "\n",
    "#open a connection(a variable used to perform operations on the file)\n",
    "file_handle = open(\"/home/didi/BGSE/semester3/text_mining_ta/additinally _sent_things/1995_Clinton_denitsa.txt\") \n",
    "#read the file using the created variable\n",
    "file_content = file_handle.read().decode(\"utf8\") \n",
    "#print file_content\n",
    "#use json module \n",
    "speech = json.loads(file_content)\n",
    "\n",
    "\n",
    "#Remind myself for lists\n",
    "\n",
    "#print speech[0]\n",
    "#print speech[1]\n",
    "#print speech[2]\n",
    "#print type(speech)\n",
    "print speech[0], speech[1]#print it libe by line\n",
    "\n",
    "#a list is mutable\n",
    "#note if you change something in speech 2 it changes in speech, too\n",
    "#speech2=speech\n",
    "#if you don't want this to happen use:\n",
    "speech2 =list(speech) #speech2 = speech[:]\n",
    "print speech2[0]\n",
    "\n",
    "#update list\n",
    "speech2[2] = \"Muh.aha\"\n",
    "print speech2[2]\n",
    "speech2[2].split(\".\")\n",
    "\n",
    "#id's check\n",
    "print id(speech)\n",
    "print id(speech2)\n",
    "\n",
    "#appending\n",
    "speech2.append([\"even\", \"a list\"])\n",
    "print speech2\n",
    "\n",
    "#insert\n",
    "speech2.insert(2,\"insert?\")\n",
    "print speech2 #insert in a given position of the list\n",
    "\n",
    "#the pop operator the last object from the list \n",
    "speech2.pop()\n",
    "type(speech[2])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that type(speech) is list. Please take a moment to go through the python list documentation and check out the various ways to manipulate lists.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2\n",
    "The first element of the list is the year of speech, the second element is the president who gave it, while the third is the transcript of the same. \n",
    "\n",
    "1. Inspect the transcript. Note the commonly used non-alphanumerical characters. Use an appropriate method of strings to get rid of them.\n",
    "2. Use an appropriate string method to split the string of the speech into a list of smaller list of words.\n",
    "3. Convert all words into lower case and return the list. Use a for loop. Then use a list comprehension to do the same.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1 \n",
    "import re #regular experessions module \n",
    "\n",
    "stripped_text = re.sub(r'([^\\s\\w]|_)+', ' ', speech[2]) \n",
    "#print stripped_text\n",
    "# \\w is for alphabetical and underscore\n",
    "# \\s for any white space character\n",
    "# I choose to put space when there is apostrophy (\"country's\" = \"country s\")because in the next \n",
    "# exercises we have to count word frequencies and I think that \"country\" and \n",
    "# \"country's\" should be counted as the same word   \n",
    "\n",
    "\n",
    "#2\n",
    "word_list = stripped_text.split()\n",
    "#print word_list\n",
    "# u stands for UNICODE string, it shouldn't affect the string  \n",
    "\n",
    "#3 \n",
    "\n",
    "\"\"\"\n",
    "The following ways two ways of writing the for loop are equivalent\n",
    "square = []\n",
    "num_list = [1, 2, 3, 4]\n",
    "for num in num_list:\n",
    "    square.append(num**2)\n",
    "num_square = [num**2 for num in num_list]\n",
    "print square, num_square\n",
    "\"\"\"\n",
    "#initialize empty list\n",
    "lower_words = list()\n",
    "# for-loop case\n",
    "list_lenght = range(len(word_list))\n",
    "for item in list_lenght:\n",
    "    lower_words.append(word_list[item].lower())\n",
    "\n",
    "# list comprehension method\n",
    "lower_words_comp = [word_list[item].lower() for item in list_lenght]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3\n",
    "Create a function _preprocess_ that takes as arguments _text_ and _non_alphanum_, a string of non-alphanumeric characters that you want get rid of. Perform all operations specfied in the previous question. However, converting to lowercase should be an optional argument. The data structure returned should be a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Inputs: text = text as a string; \n",
    "#        alphanum_char = use the RE documentation to write what characters you want to substitude with blank space\n",
    "#        lower = True by default; makes every letter lower case if True; leaves the text in original format if False \n",
    "#Outputs:list of words\n",
    "\n",
    "def preprocess(text, alphanum_chars, lower = True):\n",
    "    import re #in case we need this function outside of this notebook\n",
    "    #remove the non-numeric values \n",
    "    pattern = \"([\" + alphanum_chars + \"]|_)+\"\n",
    "    stripped_text = re.sub(pattern, ' ', text) \n",
    "    #split the words\n",
    "    word_list = stripped_text.split()\n",
    "    if  lower == True:\n",
    "        list_lenght = range(len(word_list))\n",
    "        words = [word_list[item].lower() for item in list_lenght]\n",
    "    else:\n",
    "        words = word_list\n",
    "        \n",
    "    return words \n",
    "    \n",
    "    \n",
    "#print (preprocess(speech[2],\".\", False))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "Create a function _word_freq_ that takes as input a word list that has been preprocessed and returns a dictionary of the word frequency. Which is the fourth most frequent word of your word list? (Provide code that computes it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of\n"
     ]
    }
   ],
   "source": [
    "#dictionary - not ordered; keys; finds very quickly the element; mutable \n",
    "\n",
    "# Inputs: a list of words \n",
    "# Outputs: a dictionary \n",
    "\n",
    "def wordfreq(words):\n",
    "    #create empty dictionary\n",
    "    dictionary = {}\n",
    "    #write a for-loop to count the words \n",
    "    for word in words:\n",
    "        # the get method will get the value for the apointed key \n",
    "        # and if such key is non existent will automatically \n",
    "        # put value 1  \n",
    "        dictionary[word]=dictionary.get(word,0)+1\n",
    "    return dictionary  \n",
    "    \n",
    "dictionary = wordfreq(lower_words)\n",
    "\n",
    "#Create an empty list\n",
    "empty_list=[]\n",
    "\n",
    "# Fill in the empty list\n",
    "for key, val in dictionary.items(): \n",
    "    #items() access the tuples in the dictionary\n",
    "    # note that we can loop through more than one element at the same time\n",
    "    empty_list.append((val, key))\n",
    "\n",
    "#Sort the empty list \n",
    "sorted_dictionary = sorted(empty_list)\n",
    "# Since we sorted in increasing order, \n",
    "# print the (last - 4) tuple in order to get the \n",
    "# 4th most used word and access the second item (the key)  \n",
    "print sorted_dictionary[-4][1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5\n",
    "Write a function that takes as input a word list and returns a dictionary of the frequencies of word lengths. Do not use the api collections for this assignment. But have a look at its [documentation](https://docs.python.org/2/library/collections.html). Its useful tool to have in your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 471, 2: 1793, 3: 2039, 4: 1777, 5: 860, 6: 727, 7: 633, 8: 449, 9: 285, 10: 193, 11: 98, 12: 22, 13: 26, 14: 27, 15: 1, 16: 2}\n"
     ]
    }
   ],
   "source": [
    "#Inputs: a list of words\n",
    "#Output: a dictionary of frequencies of the length of the words \n",
    "\n",
    "def wordlencount(words):\n",
    "    #create empty dictionary\n",
    "    dictionary = {}\n",
    "    #write a for-loop to count the words \n",
    "    for word in words:\n",
    "        # the get method will get the value for the apointed key \n",
    "        # and if such key is non existent will automatically \n",
    "        # put value 1  \n",
    "        dictionary[len(word)]=dictionary.get(len(word),0)+1\n",
    "        #if we want to see a particular lenght word \n",
    "        #if len(word) > 15: print(word)\n",
    "    return dictionary  \n",
    "dictionary_lenght = wordlencount(lower_words) \n",
    "print dictionary_lenght"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6\n",
    "Load the file _sou_all.txt_ in ./data/pres_speech. Inspect its contents. Familiarise yourself with using regular expressions in python. You can use this [document](https://docs.python.org/2/howto/regex.html) as a starting point. Now use regular expressions to seperate the different speeches. Your function should accept the text and a regular expression as input and return a list of lists. Each element of the list should be a list with following structure:\n",
    "\n",
    "1. year\n",
    "2. president\n",
    "3. List of the transcript of the speech broken down into paragraphs.\n",
    "\n",
    "Save your result as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#open a connection(a variable used to perform operations on the file)\n",
    "file_handle_all = open(\"/home/didi/BGSE/semester3/text_mining_ta/text_mining/data/pres_speech/sou_all.txt\") \n",
    "#read the file using the created variable\n",
    "file_content_all = file_handle_all.read().decode(\"utf8\") \n",
    "\n",
    "import re\n",
    "\n",
    "#patterns\n",
    "pattern = re.compile(\"[*]{10,}\") # 10-star pattern\n",
    "pattern2 = re.compile(\"[*]{5,}\") #5-star pattern\n",
    "pattern3= re.compile(\"[\\n]*\") #paragraph pattern\n",
    "\n",
    "splitted_content_star = pattern.split(file_content_all)\n",
    "#remove the first one\n",
    "splitted_content_star = splitted_content_star[1:]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,len(splitted_content_star)):\n",
    "    item = splitted_content_star[i]\n",
    "    \n",
    "    date = item[1:5] #characers from 2nd to 5th\n",
    "    speech_and_title_separated = pattern2.split(item[5:]) #separate based on the five stars \n",
    "    #extract the words which start with capital letters and put them in a string  \n",
    "    president_name = \" \".join(re.findall(r'[A-Z][^_]*',speech_and_title_separated[0]))\n",
    "    #separate each speech into paragraphs\n",
    "    speech_by_paragraph = pattern3.split(speech_and_title_separated[1])[1:-1] \n",
    "    #we removed the first and last because it causes 2 empty items in the list \n",
    "                              \n",
    "    splitted_content_star[i] = [date, president_name, speech_by_paragraph]\n",
    "\n",
    "\n",
    "# Write it into json file \n",
    "import json \n",
    "import io\n",
    "    \n",
    "Jsondata = json.dumps(splitted_content_star, ensure_ascii=False)\n",
    "    \n",
    "with io.open('/home/didi/BGSE/semester3/text_mining_ta/data.txt', 'w', encoding='utf8') as out_file:\n",
    "     out_file.write(unicode(Jsondata))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
